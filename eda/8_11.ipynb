{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "not_replace = True\n",
    "\n",
    "def get_app_desc(**params):\n",
    "    \"\"\"\n",
    "    返回 app_desc 文件内容\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    app_desc_data = pd.read_csv('/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/original/app_desc.dat', header=None, encoding='utf8', delimiter=' ')\n",
    "    # 以tab键分割，不知道为啥delimiter='\\t'会报错，所以先读入再分割。\n",
    "    app_desc_data = pd.DataFrame(app_desc_data[0].apply(lambda x: x.split('\\t')).tolist(), columns=['id', 'conment'])\n",
    "\n",
    "    return app_desc_data\n",
    "\n",
    "\n",
    "def get_apptype_id_name(**params):\n",
    "    \"\"\"\n",
    "    返回 apptype_id_name 文件内容\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    apptype_id_name_path = '/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/original/apptype_id_name.txt'\n",
    "    apptype_id_name_data = pd.read_table(apptype_id_name_path, header=None, sep='\\t', names=['label_code', 'label'])\n",
    "\n",
    "    return apptype_id_name_data\n",
    "\n",
    "\n",
    "def get_apptype_train(**params):\n",
    "    \"\"\"\n",
    "    返回 apptype_train 文件内容\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    apptype_train_data = pd.read_csv('/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/original/apptype_train.dat', header=None, encoding='utf8', delimiter=' ')\n",
    "    # 以tab键分割，不知道为啥delimiter='\\t'会报错，所以先读入再分割。\n",
    "    apptype_train_data = pd.DataFrame(apptype_train_data[0].apply(lambda x: x.split('\\t')).tolist(), columns=['id', 'label', 'conment'])\n",
    "\n",
    "    return apptype_train_data\n",
    "\n",
    "\n",
    "def get_app_desc_apptype(app_desc, apptype_id_name, save=True, **params):\n",
    "    \"\"\"\n",
    "    预判断app_desc的label, 根据apptype_id_name中的label是否存在于conment中，来标注label_id\n",
    "    :param app_desc:\n",
    "    :param apptype_id_name:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    if os.path.exists('/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/cache/app_desc_apptype.h5') and not_replace:\n",
    "        app_desc = reduce_mem_usage(\n",
    "            pd.read_hdf(path_or_buf='/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/cache/app_desc_apptype.h5', mode='r', key='app_desc_apptype'))\n",
    "    else:\n",
    "        index = 0\n",
    "        for i in range(apptype_id_name.shape[0]):\n",
    "            if len(str(apptype_id_name.iloc[i, 0])) < 6:\n",
    "                continue\n",
    "            else:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "        result = []\n",
    "        for raw_app_desc in range(app_desc.shape[0]):\n",
    "            if raw_app_desc % 10000 == 1:\n",
    "                print(raw_app_desc)\n",
    "\n",
    "            counts_dict = {}\n",
    "            for raw_apptype_id_name in range(index, apptype_id_name.shape[0]):\n",
    "                # 父字符串\n",
    "                father = app_desc.iloc[raw_app_desc, 1]\n",
    "                # 子字符串\n",
    "                son = apptype_id_name.iloc[raw_apptype_id_name, 1]\n",
    "                # 出现次数\n",
    "                counts = father.count(son)\n",
    "                # 如果出现次数大于等于8次， 则记录\n",
    "                if counts >= 8:\n",
    "                    counts_dict[str(apptype_id_name.iloc[raw_apptype_id_name, 0])] = counts\n",
    "\n",
    "            counts_dict_sorted = sorted(counts_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            result.append([i[0] for i in counts_dict_sorted])\n",
    "\n",
    "        app_desc['label'] = result\n",
    "\n",
    "        if save:\n",
    "            app_desc.to_hdf(path_or_buf='/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/cache/app_desc_apptype.h5', key='app_desc_apptype')\n",
    "\n",
    "    return app_desc\n",
    "\n",
    "\n",
    "def get_stopwords(**params):\n",
    "    \"\"\"\n",
    "    获取停用词文件数据\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open('/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/stopwords/stopwords.txt', 'r') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "\n",
    "def get_label1_label2(df, **params):\n",
    "    \"\"\"\n",
    "    构造label1和label2 特征\n",
    "    :param df:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df['label1'] = df['label'].apply(lambda x: x.split('|')[0])\n",
    "    df['label2'] = df['label'].apply(lambda x: x.split('|')[1] if '|' in x else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_counts_less_than_k(df, k, **params):\n",
    "    \"\"\"\n",
    "    删除出现次数少于k次的 数据\n",
    "    :param df:\n",
    "    :param k:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    counts_less_than_k = []\n",
    "\n",
    "    for key, value in df['label1'].value_counts().items():\n",
    "        if value < k:\n",
    "            counts_less_than_k.append(key)\n",
    "\n",
    "    df = df[~df['label1'].isin(counts_less_than_k)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    减少内存消耗\n",
    "    :param df:\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "            start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def store_sparse_mat(M, name, filename='store.h5'):\n",
    "    \"\"\"\n",
    "    Store a csr matrix in HDF5\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : scipy.sparse.csr.csr_matrix\n",
    "     sparse matrix to be stored\n",
    "\n",
    "    name: str\n",
    "     node prefix in HDF5 hierarchy\n",
    "\n",
    "    filename: str\n",
    "     HDF5 filename\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import sparse\n",
    "    import tables\n",
    "\n",
    "    assert (M.__class__ == sparse.csr.csr_matrix), 'M must be a csr matrix'\n",
    "    with tables.open_file(filename, 'a') as f:\n",
    "        for attribute in ('data', 'indices', 'indptr', 'shape'):\n",
    "            full_name = f'{name}_{attribute}'\n",
    "\n",
    "            # remove existing nodes\n",
    "            try:\n",
    "                n = getattr(f.root, full_name)\n",
    "                n._f_remove()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "                # add nodes\n",
    "            arr = np.array(getattr(M, attribute))\n",
    "            atom = tables.Atom.from_dtype(arr.dtype)\n",
    "            ds = f.create_carray(f.root, full_name, atom, arr.shape)\n",
    "            ds[:] = arr\n",
    "\n",
    "\n",
    "def load_sparse_mat(name, filename='store.h5'):\n",
    "    \"\"\"\n",
    "    Load a csr matrix from HDF5\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str\n",
    "     node prefix in HDF5 hierarchy\n",
    "\n",
    "    filename: str\n",
    "     HDF5 filename\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    M : scipy.sparse.csr.csr_matrix\n",
    "     loaded sparse matrix\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    import tables\n",
    "\n",
    "    with tables.open_file(filename) as f:\n",
    "        # get nodes\n",
    "        attributes = []\n",
    "        for attribute in ('data', 'indices', 'indptr', 'shape'):\n",
    "            attributes.append(getattr(f.root, f'{name}_{attribute}').read())\n",
    "            # construct sparse matrix\n",
    "    M = sparse.csr_matrix(tuple(attributes[:3]), shape=attributes[3])\n",
    "    return M\n",
    "\n",
    "\n",
    "def get_term_doc(apptype_train, app_desc, save=True, **params):\n",
    "    \"\"\"\n",
    "    获取train/test TF-IDF矩阵\n",
    "    :param df:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import jieba\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    apptype_train_term_doc_path = '/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/cache/apptype_train_term_doc.h5'\n",
    "    app_desc_term_doc_path = '/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/cache/app_desc_term_doc.h5'\n",
    "\n",
    "    if os.path.exists(apptype_train_term_doc_path) and os.path.exists(app_desc_term_doc_path) and not_replace:\n",
    "        apptype_train_term_doc = load_sparse_mat(name='apptype_train_term_doc', filename=apptype_train_term_doc_path)\n",
    "        app_desc_term_doc = load_sparse_mat(name='app_desc_term_doc', filename=app_desc_term_doc_path)\n",
    "    else:\n",
    "        stopwords = get_stopwords()\n",
    "\n",
    "        print('stopwords length:', len(stopwords))\n",
    "        apptype_train['conment'] = apptype_train['conment'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "        app_desc['conment'] = app_desc['conment'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "\n",
    "        vec = TfidfVectorizer(ngram_range=(1, 1), min_df=5, max_df=0.8, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                              stop_words=stopwords)  # 这里参数可以改\n",
    "        apptype_train_term_doc = vec.fit_transform(apptype_train['conment'])\n",
    "        app_desc_term_doc = vec.transform(app_desc['conment'])\n",
    "\n",
    "        print(type(apptype_train_term_doc))\n",
    "        if save:\n",
    "            store_sparse_mat(M=apptype_train_term_doc, name='apptype_train_term_doc',\n",
    "                             filename=apptype_train_term_doc_path)\n",
    "            store_sparse_mat(M=app_desc_term_doc, name='app_desc_term_doc', filename=app_desc_term_doc_path)\n",
    "\n",
    "    return apptype_train, app_desc, apptype_train_term_doc, app_desc_term_doc\n",
    "\n",
    "\n",
    "def get_label_encoder(apptype_train, columns: list = None, **params):\n",
    "    \"\"\"\n",
    "    返回经过labelEncoder后的数据\n",
    "    :param apptype_train:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    lbl = None\n",
    "    for column in columns:\n",
    "        # 构造标签属性\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "        # 训练\n",
    "        lbl.fit(apptype_train[column].values)\n",
    "        apptype_train[column] = lbl.transform(apptype_train[column].values)\n",
    "\n",
    "    return apptype_train, lbl\n",
    "\n",
    "\n",
    "def cross_validation(apptype_train, app_desc, apptype_train_term_doc, app_desc_term_doc, **params):\n",
    "    \"\"\"\n",
    "    k折交叉验证\n",
    "    :param apptype_train:\n",
    "    :param app_desc:\n",
    "    :param apptype_train_term_doc:\n",
    "    :param app_desc_term_doc:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # 类别数 122\n",
    "    num_class = apptype_train['label1'].max() + 1\n",
    "    # 类别\n",
    "    label = apptype_train['label1']\n",
    "\n",
    "    n_splits = 5\n",
    "    stack_train = np.zeros((apptype_train.shape[0], num_class))\n",
    "    stack_test = np.zeros((app_desc.shape[0], num_class))\n",
    "    \n",
    "    select_model = RidgeClassifier(normalize=True, max_iter=1000, random_state=2019)\n",
    "    for i, (tr, va) in enumerate(\n",
    "            StratifiedKFold(n_splits=n_splits, random_state=2019).split(apptype_train_term_doc, label)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_splits))\n",
    "        start = time.clock()\n",
    "\n",
    "        model = select_model.fit(apptype_train_term_doc[tr], label[tr])\n",
    "        score_va = model._predict_proba_lr(apptype_train_term_doc[va])\n",
    "        score_te = model._predict_proba_lr(app_desc_term_doc)\n",
    "\n",
    "        stack_train[va] += score_va\n",
    "        stack_test += score_te\n",
    "        print('consuming time:', time.clock() - start)\n",
    "\n",
    "    print(\"model acc_score:\",\n",
    "          metrics.accuracy_score(label, np.argmax(stack_train, axis=1), normalize=True, sample_weight=None))\n",
    "\n",
    "    return stack_train, stack_test\n",
    "\n",
    "\n",
    "def get_offline_accuracy(apptype_train, app_desc, stack_train, stack_test, lbl, **params):\n",
    "    \"\"\"\n",
    "    返回线下准确率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    label = apptype_train['label1']\n",
    "\n",
    "    # 获取第一第二个标签：取概率最大的前两个即可：\n",
    "    m = pd.DataFrame(stack_train)\n",
    "    first = []\n",
    "    second = []\n",
    "    for j, row in m.iterrows():\n",
    "        zz = list(np.argsort(row))\n",
    "        # 第一个标签\n",
    "        first.append(row.index[zz[-1]])\n",
    "        # 第二个标签\n",
    "        second.append(row.index[zz[-2]])\n",
    "        \n",
    "    m['label1'] = first\n",
    "    m['label2'] = second\n",
    "\n",
    "    # 计算准确率，只要命中一个就算正确：\n",
    "    k = 0\n",
    "    for i in range(len(label)):\n",
    "        if label[i] in [m.loc[i, 'label1'], m.loc[i, 'label2']]:\n",
    "            k += 1\n",
    "        else:\n",
    "            pass\n",
    "    print('线下准确率：%f' % (k / len(label)))\n",
    "\n",
    "    # 准备测试集结果：\n",
    "    results = pd.DataFrame(stack_test)\n",
    "    first = []\n",
    "    second = []\n",
    "    for j, row in results.iterrows():\n",
    "        zz = list(np.argsort(row))\n",
    "        \n",
    "    ############################# 改用新策略\n",
    "    #     zz_inverse = lbl.inverse_transform([row.index[i] for i in zz[::-1]])\n",
    "    #     at_first = zz_inverse[1]\n",
    "    #     first.append(at_first)\n",
    "    #     \n",
    "    #     for tmp in zz_inverse:\n",
    "    #         if tmp[:4] != at_first[:4]:\n",
    "    #             second.append(tmp)\n",
    "    #             break\n",
    "    # \n",
    "    # results['label1'] = first\n",
    "    # results['label2'] = second\n",
    "\n",
    "    ############################# 改用旧策略\n",
    "        # 第一个标签\n",
    "        first.append(row.index[zz[-1]])\n",
    "        # 第二个标签\n",
    "        second.append(row.index[zz[-2]])\n",
    "\n",
    "    results['label1'] = first\n",
    "    results['label2'] = second\n",
    "\n",
    "    print(\"len(list(train['label1'].values): \", len(list(apptype_train['label1'].values)))\n",
    "    print(results.head())\n",
    "\n",
    "    # 之前编码，最后逆编码回来：\n",
    "    results['label1'] = lbl.inverse_transform(results['label1'].apply(lambda x: int(x)).values)\n",
    "    results['label2'] = lbl.inverse_transform(results['label2'].apply(lambda x: int(x)).values)\n",
    "    \n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    select_model = RidgeClassifier(normalize=True, max_iter=1000, random_state=2019)\n",
    "    # 结合id列，保存：\n",
    "    print(select_model.__str__().split('(')[0])\n",
    "\n",
    "    import time\n",
    "    start = time.clock()\n",
    "\n",
    "    ####################未结合与判断\n",
    "    app_desc['label1'] = results['label1']\n",
    "    app_desc['label2'] = results['label2']\n",
    "    \n",
    "    \n",
    "    ####################结合预判断\n",
    "    # label1_list = []\n",
    "    # label2_list = []\n",
    "    # for i in range(app_desc.shape[0]):\n",
    "    #     if i % 10000 == 1:\n",
    "    #         print(i)\n",
    "    # \n",
    "    #     original = app_desc.ix[i, 'label']\n",
    "    #     original.append(results.ix[i, 'label1'])\n",
    "    #     original.append(results.ix[i, 'label2'])\n",
    "    #     label1_list.append(original[0])\n",
    "    #     label2_list.append(original[1])\n",
    "    # \n",
    "    # app_desc['label1'] = label1_list\n",
    "    # app_desc['label2'] = label2_list\n",
    "\n",
    "    print(time.clock() - start)\n",
    "\n",
    "    pd.concat([app_desc[['id', 'label1', 'label2']]], axis=1).to_csv(\n",
    "        '/home/wjunneng/Python/2019-Iflytek-Big-Data-Application-Category-Labeling-Challenge/data/submit/baseline_' + select_model.__str__().split('(')[0] + '.csv',\n",
    "        index=None,\n",
    "        encoding='utf8')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Mem. usage decreased to  3.05 Mb (0.0% reduction)\n(30000, 5)\n(1134, 5)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 获取数据\n",
    "app_desc = get_app_desc()\n",
    "apptype_id_name = get_apptype_id_name()\n",
    "apptype_train = get_apptype_train()\n",
    "\n",
    "# 预判断\n",
    "app_desc = get_app_desc_apptype(app_desc, apptype_id_name)\n",
    "\n",
    "# 获取label1/label2特征列\n",
    "apptype_train = get_label1_label2(apptype_train)\n",
    "\n",
    "print(apptype_train.shape)\n",
    "print(apptype_train[apptype_train['label2'] != 0].shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/wjunneng/Python/anaconda3/envs/lightgbm/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def add_new_apptype_train_data(df, **params):\n",
    "    \"\"\"\n",
    "    实现将具备两个label的数据转化为两条记录\n",
    "    :param df: \n",
    "    :param params: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    df_new = df[df['label2'] != 0]\n",
    "    df_new['label1'] = df['label2']\n",
    "    # 合并数据\n",
    "    df = pd.concat([df, df_new], axis=0)\n",
    "    # label2列清空\n",
    "    df['label2'] = 0\n",
    "   \n",
    "    return df\n",
    "\n",
    "# 若label2中存在数据则新添加\n",
    "apptype_train = add_new_apptype_train_data(apptype_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "stack:1/5\n",
      "consuming time: 128.43343099999998\nstack:2/5\n",
      "consuming time: 106.55498599999999\nstack:3/5\n",
      "consuming time: 78.92164000000002\nstack:4/5\n",
      "consuming time: 82.228141\nstack:5/5\n",
      "consuming time: 119.29692399999999\nmodel acc_score: 0.5853948467519116\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 删除出现次数少于5次的数据\n",
    "k = 5\n",
    "apptype_train = delete_counts_less_than_k(apptype_train, k)\n",
    "\n",
    "# 获取TF-IDF矩阵\n",
    "apptype_train, app_desc, apptype_train_term_doc, app_desc_term_doc = get_term_doc(apptype_train, app_desc)\n",
    "\n",
    "# 对label1进行labelEncoder\n",
    "apptype_train, lbl = get_label_encoder(apptype_train, columns=['label1'])\n",
    "\n",
    "# 交叉验证\n",
    "stack_train, stack_test = cross_validation(apptype_train, app_desc, apptype_train_term_doc, app_desc_term_doc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "线下准确率：0.734466\n",
      "len(list(train['label1'].values):  31126\n          0         1         2         3         4         5         6  \\\n0  0.040030  0.040807  0.039443  0.040629  0.039916  0.039650  0.040332   \n1  0.042326  0.038572  0.039257  0.040822  0.052533  0.040952  0.040451   \n2  0.040081  0.038822  0.041569  0.040094  0.039838  0.039695  0.040242   \n3  0.043255  0.062543  0.040487  0.043342  0.052526  0.041322  0.039868   \n4  0.039883  0.041008  0.038915  0.040388  0.040680  0.039536  0.040564   \n\n          7         8         9  ...       114       115       116       117  \\\n0  0.041359  0.041478  0.038967  ...  0.040359  0.039460  0.039731  0.040588   \n1  0.042091  0.037940  0.038592  ...  0.040214  0.039486  0.042883  0.040361   \n2  0.040408  0.039000  0.039277  ...  0.040328  0.039997  0.037359  0.040263   \n3  0.039872  0.077870  0.048080  ...  0.040382  0.038988  0.038825  0.040860   \n4  0.039842  0.040152  0.037994  ...  0.040516  0.039627  0.040437  0.039494   \n\n        118       119       120       121  label1  label2  \n0  0.041332  0.038785  0.039944  0.040640      48      19  \n1  0.045925  0.043995  0.039106  0.040211     107     113  \n2  0.040398  0.038356  0.040300  0.038840      70      30  \n3  0.039894  0.039297  0.038870  0.038933       8       1  \n4  0.039714  0.040280  0.040985  0.039824      70     103  \n\n[5 rows x 124 columns]\nRidgeClassifier\n0.0013400000000274304\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 线下准确率+测试结果\n",
    "get_offline_accuracy(apptype_train, app_desc, stack_train, stack_test, lbl)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-e50da876",
   "language": "python",
   "display_name": "PyCharm (ForecastScore)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}